###################################################################
#                            Melissa                              #
#-----------------------------------------------------------------#
#   COPYRIGHT (C) 2017  by INRIA and EDF. ALL RIGHTS RESERVED.    #
#                                                                 #
# This source is covered by the BSD 3-Clause License.             #
# Refer to the  LICENCE file for further information.             #
#                                                                 #
#-----------------------------------------------------------------#
#  Original Contributors:                                         #
#    Theophile Terraz,                                            #
#    Bruno Raffin,                                                #
#    Alejandro Ribes,                                             #
#    Bertrand Iooss,                                              #
###################################################################


"""
    Study main module
"""

from threading import Thread
from sys import exit
import os
import time
import copy
import imp
import numpy as np
import logging
import traceback
from ctypes import cdll, create_string_buffer, c_char_p, c_wchar_p
imp.load_source('simulation', '@CMAKE_INSTALL_PREFIX@/share/melissa/launcher/simulation.py')
from simulation import Server
#from simulation import SingleSimuGroup
from simulation import MultiSimuGroup
from simulation import Group
from simulation import SobolGroup
from simulation import Job

melissa_message = cdll.LoadLibrary('@CMAKE_INSTALL_PREFIX@/lib/libmelissa_message.so')
melissa_message.bind_message_rcv.argtypes = [c_char_p]
melissa_message.bind_message_resp.argtypes = [c_char_p]
melissa_message.bind_message_snd.argtypes = [c_char_p]
melissa_message.send_resp_message.argtypes = [c_char_p]

# Jobs and executions status
from simulation import NOT_SUBMITTED
from simulation import WAITING
from simulation import PENDING
from simulation import RUNNING
from simulation import FINISHED
from simulation import TIMEOUT

logging.basicConfig(format='%(asctime)s %(message)s',
                    datefmt='%m/%d/%Y %I:%M:%S %p',
                    filename='melissa_launcher.log',
                    filemode='w',
                    level=logging.DEBUG)
groups = list()
server = Server()

class StateChecker(Thread):
    """
        Thread in charge of recieving Server messages
    """
    def __init__(self):
        Thread.__init__(self)
        self.running_study = True

    def run(self):
        try:
            while self.running_study:
                time.sleep(1)
                with server.lock:
                    server.check_job()
                for group in groups:
                    with group.lock:
                        if (group.job_status < FINISHED and
                                group.job_status > NOT_SUBMITTED):
                            s = copy.deepcopy(group.job_status)
                            group.check_job()
                            if s <= PENDING and group.job_status == RUNNING:
                                logging.info('group ' + str(group.rank) + ' job started')
                                group.start_time = time.time()
                            elif s <= RUNNING and group.job_status == FINISHED:
                                logging.info('end job group ' + str(group.rank))
                                group.finalize()
            logging.info('closing state checker process')
        except:
            print('=== State checker thread crashed ===')
            logging.error(traceback.print_exc())
            return

class Messenger(Thread):
    """
        Thread in charge of checking job status
    """
    def __init__(self, batch_size = 1):
        Thread.__init__(self)
        self.running_study = True
        self.batch_size = batch_size

    def run(self):
        try:
            last_server = 0
            last_msg_to_server = 0
            while self.running_study:
#                if np.random.rand() < 0.2: print('r' + 1)
                buff = create_string_buffer(melissa_message.melissa_get_message_len())
                melissa_message.wait_message(buff)
                if buff.value.decode() != 'nothing':
                    logging.debug('message: '+buff.value.decode())
                last_server = time.time()
                message = buff.value.decode().split()
                if message[0] == 'stop':
                    with server.lock:
                        server.status = FINISHED # finished
                        logging.info('end study')
                elif message[0] == 'timeout':
                    if message[1] != '-1':
                        group_id = int(message[1])//self.batch_size
                        logging.info('restarting group ' + str(group_id)
                                     + ' (timeout detected by server)')
                        with groups[group_id].lock:
                            groups[group_id].restart()
                            groups[group_id].job_status = PENDING
                elif message[0] == 'group_state':
                    group_id = int(message[1])//self.batch_size
                    simu_id = int(message[1])%self.batch_size
                    with groups[group_id].lock:
                        if groups[group_id].job_type == 3:
                            if groups[group_id].status < 2:
                                groups[group_id].status = int(message[2])
                            #print("group " +str(group_id)+ " simu "+str(simu_id))
                            groups[group_id].simu_status[simu_id] = int(message[2])
                            if all(simu_status == 2 for simu_status in groups[group_id].simu_status):
                                groups[group_id].status = 2
                                #print('Group ' + str(group_id) + ' status: ' + str(groups[group_id].status))
                        else:
                            groups[group_id].status = int(message[2])
                        if groups[group_id].status == 2:
                            groups[group_id].finalize()
                    if message[2] == 2:
                        logging.info('Group ' + message[1] + ' finished')
                    if message[2] == 1:
                        logging.info('Group ' + message[1] + ' connected to server')
                elif message[0] == 'server':
                    with server.lock:
                        server.status = RUNNING
                        server.job_status = RUNNING
                        server.node_name = message[1]
                        logging.info('Melissa Server node name: ' +
                                     str(server.node_name) + '; '+
                                     'Melissa Server job id: ' +
                                     str(server.job_id))

                if last_server > 0:
                    if (time.time() - last_server) > 100:
                        logging.info('server timeout\n')
                        with server.lock:
                            server.status = TIMEOUT
                if (time.time() - last_msg_to_server) > 10:
                    buff.value = str('hello server '+str(time.time())).encode()
                    melissa_message.send_message(buff)
                    last_msg_to_server = time.time()
            logging.info('closing messenger thread')
        except:
            print('=== Messenger thread crashed ===')
            logging.error(traceback.print_exc())
            return

class Responder(Thread):
    """
        Thread in charge of responding to server requests
    """
    def __init__(self, batch_size = 1):
        Thread.__init__(self)
        self.running_study = True
        self.batch_size = batch_size

    def run(self):
        try:
            while self.running_study:
                buff = create_string_buffer(melissa_message.melissa_get_message_len())
                melissa_message.get_resp_message(buff)
                if buff.value.decode != 'nothing':
                    logging.debug('message: '+buff.value.decode())
                    print('message: '+buff.value.decode())
                message = buff.value.decode().split()
                if message[0] == 'simu_info':
                    group = int(message[1])//self.batch_size
                    simu = int(message[1])%self.batch_size
                    with groups[group].lock:
                        if groups[group].job_type == 4:
                            message = "job "+str(groups[group].rank)+" " +str(groups[group].job_id) + " " + " ".join(str(j) for j in groups[group].param_set[0]) + "\000"
                        elif groups[group].job_type == 3:
                            message = "job "+str(groups[group].simu_id[simu])+" " +str(groups[group].job_id) + " " + " ".join(str(j) for j in groups[group].param_set[simu]) + "\000"
                        elif groups[group].job_type == 2:
                            message = "job "+str(groups[group].rank)+" " +str(groups[group].job_id) + " " + " ".join(str(j) for j in groups[group].param_set) + "\000"
                    melissa_message.send_resp_message(message.encode())
                if message[0] == 'test_timeout':
                    melissa_message.send_resp_message("alive".encode())

            logging.info('closing responder thread')
        except:
            print('=== Responder thread crashed ===')
            logging.error(traceback.print_exc())
            return

class Server_user_functions(object):
    """
        Options for the simulations
    """
    def __init__(self, parent):
        self.parent = parent

    def create(self, func_ptr):
        self.parent.usr_func['create_server'] = func_ptr

    def launch(self, func_ptr):
        self.parent.usr_func['launch_server'] = func_ptr

    def check_job(self, func_ptr):
        self.parent.usr_func['check_server_job'] = func_ptr

    def restart(self, func_ptr):
        self.parent.usr_func['restart_server'] = func_ptr

    def cancel_job(self, func_ptr):
        self.parent.usr_func['cancel_server_job'] = func_ptr

    def finalize(self, func_ptr):
        self.parent.usr_func['finalize_server'] = func_ptr

class Simulation_user_functions(object):
    """
        Options for the simulations
    """
    def __init__(self, parent):
        self.parent = parent

    def create(self, func_ptr):
        self.parent.usr_func['create_group'] = func_ptr

    def launch(self, func_ptr):
        self.parent.usr_func['launch_group'] = func_ptr

    def check_job(self, func_ptr):
        self.parent.usr_func['check_group_job'] = func_ptr

    def restart(self, func_ptr):
        self.parent.usr_func['restart_group'] = func_ptr

    def cancel_job(self, func_ptr):
        self.parent.usr_func['cancel_group_job'] = func_ptr

    def finalize(self, func_ptr):
        self.parent.usr_func['finalize_group'] = func_ptr

class Study(object):
    """
        Study class, containing instances of the threads
    """
    def __init__(self, stdy_opt = {}, ml_stats = {}, usr_func = {}):
        self.nb_param = 0
        self.groups = list()
        self.sobol = False
        self.stdy_opt = stdy_opt
        self.ml_stats = ml_stats
        self.usr_func = usr_func
        self.threads = {}
        self.server = Server_user_functions(self)
        self.simulation = Simulation_user_functions(self)

    # options
    def set_option(self, option_key, option_value):
        self.stdy_opt[option_key] = option_value

    def get_option(self, option_key):
        return self.stdy_opt[option_key]

    def set_threshold_values(self, threshold_values):
        self.stdy_opt['threshold_values'] = threshold_values

    def get_threshold_values(self):
        return self.stdy_opt['threshold_values']

    def set_quantile_values(self, quantile_values):
        self.stdy_opt['quantile_values'] = quantile_values

    def get_quantile_values(self):
        return self.stdy_opt['quantile_values']

    def set_working_directory(self, working_directory):
        self.stdy_opt['working_directory'] = working_directory

    def get_working_directory(self):
        return self.stdy_opt['working_directory']

    def set_sampling_size(self, sampling_size):
        self.stdy_opt['sampling_size'] = sampling_size

    def get_sampling_size(self):
        return self.stdy_opt['sampling_size']

    def set_nb_timesteps(self, nb_timesteps):
        self.stdy_opt['nb_timesteps'] = nb_timesteps

    def get_nb_timesteps(self):
        return self.stdy_opt['nb_timesteps']

    def set_field_names(self, field_names):
        self.stdy_opt['field_names'] = field_names

    def get_field_names(self):
        return self.stdy_opt['field_names']

    def set_simulation_timeout(self, simulation_timeout):
        self.stdy_opt['simulation_timeout'] = simulation_timeout

    def get_simulation_timeout(self):
        return self.stdy_opt['simulation_timeout']

    def set_checkpoint_interval(self, checkpoint_interval):
        self.stdy_opt['checkpoint_interval'] = checkpoint_interval

    def get_checkpoint_interval(self):
        return self.stdy_opt['checkpoint_interval']

    def set_coupling(self, coupling):
        self.stdy_opt['coupling'] = coupling

    def get_coupling(self):
        return self.stdy_opt['coupling']

    def set_verbosity(self, verbosity):
        self.stdy_opt['verbosity'] = verbosity

    def get_verbosity(self):
        return self.stdy_opt['verbosity']

    def set_batch_size(self, batch_size):
        self.stdy_opt['batch_size'] = batch_size

    def get_batch_size(self):
        return self.stdy_opt['batch_size']

    def set_param_distribution(self, param_distribution):
        self.stdy_opt['param_distribution'] = param_distribution

    def get_param_distribution(self):
        return self.stdy_opt['param_distribution']

    # stats
    def compute_stat(self, stat_key):
        self.ml_stats[stat_key] = True

    def compute_mean(self):
        self.ml_stats['mean'] = True

    def compute_variance(self):
        self.ml_stats['variance'] = True

    def compute_skewness(self):
        self.ml_stats['skewness'] = True

    def compute_kurtosis(self):
        self.ml_stats['kurtosis'] = True

    def compute_min(self):
        self.ml_stats['min'] = True

    def compute_max(self):
        self.ml_stats['max'] = True

    def compute_threshold_exceedances(self, threshold_values):
        self.stdy_opt['threshold_values'] = threshold_values
        self.ml_stats['threshold_exceedance'] = True

    def compute_quantiles(self, quantile_values):
        self.stdy_opt['quantile_values'] = quantile_values
        self.ml_stats['quantile'] = True

    def compute_sobol_indices(self, coupling = "MELISSA_COUPLING_DEFAULT"):
        self.stdy_opt['coupling'] = coupling
        self.ml_stats['sobol_indices'] = True

    # functions
    def set_function(self, func_key, func_ptr):
        self.usr_func[func_key] = func_ptr

    def check_job(self, func_ptr):
        self.usr_func['check_job'] = func_ptr

    def check_scheduler_load(self, func_ptr):
        self.usr_func['check_scheduler_load'] = func_ptr

    def cancel_job(self, func_ptr):
        self.usr_func['cancel_job'] = func_ptr

    def postprocessing(self, func_ptr):
        self.usr_func['postprocessing'] = func_ptr

    def finalize(self, func_ptr):
        self.usr_func['finalize'] = func_ptr

    def run(self):
        """
            Main study method
        """
        if not os.path.isdir(self.stdy_opt['working_directory']):
            os.mkdir(self.stdy_opt['working_directory'])
        os.chdir(self.stdy_opt['working_directory'])

        nb_errors = self.check_options()
        if nb_errors > 0:
            logging.error(str(nb_errors) + ' errors in options')
            exit()

        Job.set_usr_func(self.usr_func)
        Job.set_stdy_opt(self.stdy_opt)
        Job.set_ml_stats(self.ml_stats)
        Job.set_nb_param(self.nb_param)

        logging.debug('create threads dict')
        self.threads['messenger'] = Messenger(self.stdy_opt['batch_size'])
        self.threads['state_checker'] = StateChecker()
        self.threads['responder'] = Responder(self.stdy_opt['batch_size'])

        create_study(self.usr_func)
        self.create_group_list()
        # init zmq context
        melissa_message.init_context()
        logging.debug('bind to recv port '+str(self.stdy_opt['recv_port']))
        print('bind to recv port '+str(self.stdy_opt['recv_port']))
        melissa_message.bind_message_rcv(str(self.stdy_opt['recv_port']).encode())
        melissa_message.bind_message_resp(str(self.stdy_opt['resp_port']).encode())
        logging.info('submit server')

        server.set_nb_param(self.nb_param)
        server.set_path(self.stdy_opt['working_directory'])
        server.create_options()
        try:
            server.launch()
        except:
            logging.error('Error while launching server')
            print('=== Error while launching server ===')
            logging.error(traceback.print_exc())
            self.stop()
        logging.debug('start messenger thread')
        self.threads['messenger'].start()
        logging.debug('wait server start')
        try:
            server.wait_start()
        except:
            logging.error('Error while waiting server')
            print('=== Error while waiting server ===')
            logging.error(traceback.print_exc())
            self.stop()
        server.write_node_name()
        # connect to server
        logging.debug('connect to server port '+str(self.stdy_opt['send_port']))
        #melissa_message.connect_message_snd(str(server.node_name), str(self.stdy_opt['send_port']))
        melissa_message.bind_message_snd(str(self.stdy_opt['send_port']).encode())
        logging.debug('start status checker thread')
        self.threads['state_checker'].start()
        if self.stdy_opt['learning']:
            self.threads['responder'].start()
        for group in groups:
            self.fault_tolerance()
            while check_scheduler_load(self.usr_func) == False:
                time.sleep(1)
                self.fault_tolerance()
            logging.info('submit group '+str(group.rank))
            try:
                group.server_node_name = str(server.node_name)
                group.launch()
            except:
                logging.error('Error while launching group')
                print('=== Error while launching group ===')
                logging.error(traceback.print_exc())
                self.stop()
        time.sleep(1)
        while (server.status != FINISHED
               or any([i.status != FINISHED for i in groups])):
            self.fault_tolerance()
            time.sleep(1)
        time.sleep(1)
        server.finalize()
        self.stop()

    def stop(self):
        for group in groups:
            if group.status < FINISHED and group.status > NOT_SUBMITTED:
                with group.lock:
                    group.cancel()
        if server.job_status < FINISHED:
            server.cancel()
        self.threads['messenger'].running_study = False
        if self.stdy_opt['learning']:
            self.threads['responder'].running_study = False
        self.threads['state_checker'].running_study = False
        self.threads['messenger'].join()
        if self.stdy_opt['learning']:
            self.threads['responder'].join()
        self.threads['state_checker'].join()
        postprocessing(self.usr_func)
        finalize(self.usr_func)
        # finalize zmq context
        melissa_message.close_message()
        exit()

    def check_options(self):
        """
            Validates user provided options
        """
        errors = 0
        if (not 'sobol_indices' in self.ml_stats.keys()):
            self.ml_stats['sobol_indices'] = False

        test_parameters = draw_parameter_set(self.usr_func, self.stdy_opt)
        self.nb_param = len(test_parameters)

        if not self.ml_stats['sobol_indices'] and self.nb_param < 1:
            logging.error('Error bad option: not enough parameters')
            errors += 1
        if self.ml_stats['sobol_indices'] and self.nb_param < 2:
            logging.error('Error bad option: not enough parameters')
            errors += 1
        if self.stdy_opt['sampling_size'] < 2:
            logging.error('Error bad option: sample_size not big enough')
            errors += 1

        if (not 'verbosity' in self.stdy_opt):
            self.stdy_opt['verbosity'] = 2

        if (not 'batch_size' in self.stdy_opt):
            self.stdy_opt['batch_size'] = 1

        if (not 'send_port' in self.stdy_opt):
            self.stdy_opt['send_port'] = 5556

        if (not 'recv_port' in self.stdy_opt):
            self.stdy_opt['recv_port'] = 5555

        if (not 'resp_port' in self.stdy_opt):
            self.stdy_opt['resp_port'] = 5554

        if (not 'data_port' in self.stdy_opt):
            self.stdy_opt['data_port'] = 2006

        if (not 'learning' in self.stdy_opt):
            self.stdy_opt['learning'] = False
        else:
            if (not 'nn_path' in self.stdy_opt):
                self.stdy_opt['nn_path']="@CMAKE_INSTALL_PREFIX@/lib"


        if (self.ml_stats['sobol_indices']):
            self.stdy_opt['batch_size'] = 1
            self.sobol = True

        if self.stdy_opt['send_port'] == self.stdy_opt['recv_port']:
            logging.error('Error: send_port and recv_port can not have the same value')
            errors += 1

        if 'threshold_exceedance' in self.ml_stats:
            self.ml_stats['threshold_exceedances'] = self.ml_stats['threshold_exceedance']
            self.ml_stats['threshold_exceedance'] = False

        if 'threshold_value' in self.stdy_opt:
            self.stdy_opt['threshold_values'] = self.stdy_opt['threshold_value']
        elif 'threshold_value' in self.ml_stats:
            self.stdy_opt['threshold_values'] = self.ml_stats['threshold_value']
            self.ml_stats['threshold_value'] = False
        elif 'threshold_values' in self.ml_stats:
            self.stdy_opt['threshold_values'] = self.ml_stats['threshold_values']
            self.ml_stats['threshold_values'] = False

        if type(self.stdy_opt['threshold_values']) not in (list, tuple, set):
            self.stdy_opt['threshold_values'] = [self.stdy_opt['threshold_values']]

        if 'quantile' in self.ml_stats:
            self.ml_stats['quantiles'] = self.ml_stats['quantile']
            self.ml_stats['quantile'] = False

        if 'quantile_value' in self.stdy_opt:
            self.stdy_opt['quantile_values'] = self.stdy_opt['quantile_value']
        elif 'quantile_value' in self.ml_stats:
            self.stdy_opt['quantile_values'] = self.ml_stats['quantile_value']
            self.ml_stats['quantile_value'] = False
        elif 'quantile_values' in self.ml_stats:
            self.stdy_opt['quantile_values'] = self.ml_stats['quantile_values']
            self.ml_stats['quantile_values'] = False

        if type(self.stdy_opt['threshold_values']) not in (list, tuple, set):
            self.stdy_opt['threshold_values'] = [self.stdy_opt['threshold_values']]

        if not 'check_server_job' in self.usr_func.keys():
            if 'check_job' in self.usr_func.keys():
                self.usr_func['check_server_job'] = self.usr_func['check_job']

        if not 'check_group_job' in self.usr_func.keys():
            if 'check_job' in self.usr_func.keys():
                self.usr_func['check_group_job'] = self.usr_func['check_job']

        if not 'cancel_server_job' in self.usr_func.keys():
            if 'cancel_job' in self.usr_func.keys():
                self.usr_func['cancel_server_job'] = self.usr_func['cancel_job']

        if not 'cancel_group_job' in self.usr_func.keys():
            if 'cancel_job' in self.usr_func.keys():
                self.usr_func['cancel_group_job'] = self.usr_func['cancel_job']


        optional_func = ['create_study',
                         'create_group',
                         'restart_server',
                         'restart_group',
                         'check_scheduler_load',
                         'postprocessing',
                         'finalize']
        mandatory_func = ['launch_server',
                          'launch_group',
                          'check_server_job',
                          'check_group_job',
                          'cancel_server_job',
                          'cancel_group_job']

        for func_name in optional_func:
            if not func_name in self.usr_func.keys():
                self.usr_func[func_name] = None
                logging.warning('Warning: no \''+func_name+'\' key in USER_FUNCTIONS')
            elif self.usr_func[func_name] is None:
                logging.warning('Warning: no \''+func_name+'\' provided')

        for func_name in mandatory_func:
            if not func_name in self.usr_func.keys():
                logging.error('Error: no \''+func_name+'\' key in USER_FUNCTIONS')
                errors += 1

        return errors

    def create_group_list(self):
        """
            Job list creation
        """
        global groups
        Group.reset()

        input_parameters = draw_parameter_set(self.usr_func, self.stdy_opt)
        if self.sobol:
            input_parameters2 = draw_parameter_set(self.usr_func, self.stdy_opt)
            for i in range(self.stdy_opt['sampling_size']):
                self.groups.append(SobolGroup(
                    input_parameters[i],
                    input_parameters2[i]))
        else:
            for i in range(self.stdy_opt['sampling_size']//self.stdy_opt['batch_size']):
                param_sets = list()
                for j in range(self.stdy_opt['batch_size']):
                    param_sets.append(draw_parameter_set(self.usr_func, self.stdy_opt))
                self.groups.append(MultiSimuGroup(param_sets))

        for group in self.groups:
            group.create()
        groups = self.groups

    def fault_tolerance(self):
        """
            Compares job status and study status, restart crashed groups
        """

        # check if threads are still alive
        if not self.threads['messenger'].isAlive():
            logging.error('Messenger thread crashed')
            self.threads['messenger'].join()
            self.threads['messenger'] = Messenger(self.stdy_opt['batch_size'])
            self.threads['messenger'].start()
        if not self.threads['state_checker'].isAlive():
            logging.error('State checker thread crashed')
            self.threads['state_checker'].join()
            self.threads['state_checker'] = StateChecker()
            self.threads['state_checker'].start()
        if self.stdy_opt['learning']:
            if not self.threads['responder'].isAlive():
                logging.error('Responder thread crashed')
                self.threads['responder'].join()
                self.threads['responder'] = Responder()
                self.threads['responder'].start()

        sleep = False
        with server.lock:
            if server.status != RUNNING or server.job_status != RUNNING:
                sleep = True
        if sleep == True:
            time.sleep(3)
            sleep = False

        if (((server.status != RUNNING or server.job_status != RUNNING) and server.status != FINISHED)
                and not all([i.status == FINISHED for i in groups])):
            print('server status: '+str(server.status)+' job_status: '+str(server.job_status))
            for group in groups:
                if group.status < FINISHED and group.status > NOT_SUBMITTED:
                    with group.lock:
                        group.cancel()
            logging.info('resubmit server job')
            time.sleep(1)
            try:
                server.restart()
            except:
                logging.error('Error while restarting server')
                print('=== Error while restarting server ===')
                logging.error(traceback.print_exc())
                self.stop()
            logging.debug('wait server restart')
            try:
                server.wait_start()
            except:
                logging.error('Error while waiting server')
                print('=== Error while waiting server ===')
                logging.error(traceback.print_exc())
                self.stop()

            logging.info('server start')
            server.write_node_name()
            # connect to server
            #logging.debug('connect to server port '+str(self.stdy_opt['send_port']))
            #melissa_message.connect_message_snd(str(server.node_name), str(self.stdy_opt['send_port']))

            time.sleep(1)
            for group in groups:
                if group.status < FINISHED and group.status > NOT_SUBMITTED:
                    with group.lock:
                        logging.info('resubmit group ' + str(group.rank)
                                     + ' (server crash)')
                        try:
                            group.server_node_name = str(server.node_name)
                            group.restart()
                        except:
                            logging.error('Error while restarting group '+group.rank)
                            print('=== Error while restarting group '+group.rank+' ===')
                            logging.error(traceback.print_exc())
                            self.stop()
            time.sleep(2)

        for group in groups:
            if group.status > NOT_SUBMITTED and group.status < FINISHED:
                with group.lock:
                    if group.status <= RUNNING and group.job_status == FINISHED:
                        sleep = True
            if sleep == True:
                time.sleep(10)
                sleep = False
                with group.lock:
                    if group.status <= RUNNING:
                        logging.info("resubmit group " + str(group.rank)
                                     + " (simulation crashed)")
                        try:
                            group.restart()
                        except:
                            logging.error('Error while restarting group '+str(group.rank))
                            print('=== Error while restarting group '+str(group.rank)+' ===')
                            logging.error(traceback.print_exc())
                            self.stop()
            with group.lock:
                if group.status == WAITING:
                    if group.job_status == RUNNING:
                        if time.time() - group.start_time > self.stdy_opt['simulation_timeout']:
                            logging.info("resubmit group " + str(group.rank)
                                         + " (timeout detected by launcher)")
                            try:
                                group.restart()
                            except:
                                logging.error('Error while restarting group '+group.rank)
                                print('=== Error while restarting group '+group.rank+' ===')
                                logging.error(traceback.print_exc())
                                self.stop()
#    time.sleep(1)


def create_study(usr_func):
    """
        Creates study environment
    """
    if "create_study" in usr_func.keys() \
    and usr_func['create_study']:
        try:
            usr_func['create_study']()
        except:
            logging.warning("Create study failed")
            traceback.print_exc()
    else:
        pass

def draw_parameter_set(usr_func, stdy_opt):
    """
        Draws a set of parameters using user defined function
    """
    if usr_func['draw_parameter_set']:
        try:
            param_set = usr_func['draw_parameter_set']()
        except:
            logging.error("Draw parameter set failed")
            logging.error(traceback.print_exc())
            exit()
    else:
        param_set = np.zeros(stdy_opt['nb_parameters'])
        for i in range(stdy_opt['nb_parameters']):
            param_set[i] = np.random.uniform(0, 1)
    return param_set

def check_scheduler_load(usr_func):
    """
        Return False if the load is full
    """
    if "check_scheduler_load" in usr_func.keys() \
    and usr_func['check_scheduler_load']:
        try:
            return usr_func['check_scheduler_load']()
        except:
            logging.warning("Check scheduler load failed")
            logging.error(traceback.print_exc())
            return True
    else:
        return True

# Study end #

def postprocessing(usr_func):
    """
        User defined postprocessing
    """
    if "postprocessing" in usr_func.keys() \
    and usr_func['postprocessing']:
        try:
            usr_func['postprocessing']()
        except:
            logging.warning("Postprocessing failed")
            traceback.print_exc()
    else:
        pass

def finalize(usr_func):
    """
        User defined final step
    """
    if "finalize" in usr_func.keys() \
    and usr_func['finalize']:
        try:
            usr_func['finalize']()
        except:
            logging.warning("Finalize failed")
            logging.error(traceback.print_exc())
    else:
        pass

